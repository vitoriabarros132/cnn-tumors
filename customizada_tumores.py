# -*- coding: utf-8 -*-
"""customizada - tumores.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17WR61yDyLu-1fGLjWcKdMdxyiQQYqaRz
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("masoudnickparvar/brain-tumor-mri-dataset")

print("Path to dataset files:", path)

# Link do Colab com o Drive
from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf  # Biblioteca para criar a rede neural
from tensorflow import keras
from keras import callbacks
from keras import layers
from keras import regularizers
from keras.callbacks import ModelCheckpoint
import matplotlib.pyplot as plt  # Para visualizar imagens
import numpy as np  # Para manipulação de arrays
import os  # Para trabalhar com diretórios
from sklearn.model_selection import StratifiedKFold # Importar para validação cruzada

# Definir o caminho do dataset
dataset_path = "/kaggle/input/brain-tumor-mri-dataset"
train_dir = os.path.join(dataset_path, "Training")
test_dir = os.path.join(dataset_path, "Testing")

# Coletar caminhos de imagem e rótulos para validação cruzada
image_paths = []
labels = []
class_names = sorted(os.listdir(train_dir))
num_classes = len(class_names)
class_to_label = {name: i for i, name in enumerate(class_names)}

for class_name in class_names:
    class_dir = os.path.join(train_dir, class_name)
    for img_name in os.listdir(class_dir):
        image_paths.append(os.path.join(class_dir, img_name))
        labels.append(class_to_label[class_name])

# Converter para numpy arrays
image_paths = np.array(image_paths)
labels = np.array(labels)

# Carregar o conjunto de teste separadamente
test_dataset = keras.preprocessing.image_dataset_from_directory(
    test_dir,
    image_size=(227, 227),
    batch_size=32, # Ajustar o tamanho do batch
    shuffle=False,
    label_mode='categorical') # São mais de 2 classes, então é categorical

# Otimização de desempenho para o conjunto de teste
AUTOTUNE = tf.data.AUTOTUNE
test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)


# Data Augmentation
data_augmentation = keras.Sequential(
    [
        layers.RandomFlip("horizontal"), # Inversão horizontal
        layers.RandomRotation(0.1),       # Rotação de até 10% (0.1 radianos)
        layers.RandomZoom(0.1),         # Zoom de até 10%
        layers.RandomTranslation(height_factor=0.1, width_factor=0.1), # Deslocamento
       layers.RandomContrast(0.1),     # Ajuste de contraste (opcional)
    ],
    name="data_augmentation",
)

# Função para criar o modelo (para reutilizar em cada fold)
def create_model(num_classes):
    model = keras.Sequential([
        keras.Input(shape=(227, 227, 3)),
        data_augmentation,
        layers.Rescaling(1./255),
        layers.Conv2D(32, (3,3), activation='relu'),
        layers.MaxPooling2D(2,2), layers.Dropout(0.3),
        layers.Conv2D(64, (3,3), activation='relu'),
        layers.MaxPooling2D(2,2), layers.Dropout(0.2),
        layers.Conv2D(128, (3,3), activation='relu'),
        layers.MaxPooling2D(2,2), layers.Dropout(0.3),
        layers.Flatten(),
        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.003)), layers.Dropout(0.3),
        layers.Dense(num_classes, activation='softmax')
    ])
    model.compile(
        loss="categorical_crossentropy",
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        metrics=["accuracy"])
    return model

# Configurar K-Fold
K = 5 # Número de folds (ajustável, mas normalmente usar entre 5 e 10)
skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)

fold = 0
histories = []
test_losses = []
test_accuracies = []

for train_index, val_index in skf.split(image_paths, labels):
    fold += 1
    print(f"Processing Fold {fold}/{K}")

    # Obter caminhos de imagem e rótulos para o fold atual
    train_paths, val_paths = image_paths[train_index], image_paths[val_index]
    train_labels, val_labels = labels[train_index], labels[val_index]

    # Criar datasets de treino e validação para o fold
    def create_tf_dataset(image_paths, labels, num_classes, batch_size, shuffle=False, augment=False):
        # Função auxiliar para carregar e pré-processar imagens
        def load_image(image_path, label):
            img = tf.io.read_file(image_path)
            img = tf.image.decode_jpeg(img, channels=3)
            img = tf.image.resize(img, [227, 227])
            label = tf.one_hot(label, num_classes)
            return img, label

        dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))
        if shuffle:
            dataset = dataset.shuffle(buffer_size=1000)
        dataset = dataset.map(lambda x, y: load_image(x, y), num_parallel_calls=AUTOTUNE)
        if augment:
             dataset = dataset.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE) # Aplica DA no treino
        dataset = dataset.batch(batch_size)
        dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)
        return dataset

    train_dataset_fold = create_tf_dataset(train_paths, train_labels, num_classes, batch_size=32, shuffle=True, augment=True)
    val_dataset_fold = create_tf_dataset(val_paths, val_labels, num_classes, batch_size=32)


    # Criação e compilação do modelo para o fold atual
    model = create_model(num_classes)

    # Early Stopping e Learning Rate para o fold atual
    early_stopping = callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    )

    reduce_lr = callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.2,
        patience=5,
        min_lr=0.00001
    )

    # Callback para salvar o melhor modelo de cada fold
    model_checkpoint_callback_fold = ModelCheckpoint(
        filepath=f'/content/drive/MyDrive/PROJETO IA 2024-2025/modelos_salvos/best_model_fold_{fold}.keras',
        monitor='val_loss',
        save_best_only=True,
        mode='min',
        verbose=1
    )


    print(f"Training Fold {fold}/{K}...")
    history = model.fit(
        train_dataset_fold,
        epochs=50, # O early stopping irá parar quando necessário
        validation_data=val_dataset_fold, # Passar dados de validação para o monitoramento correto
        callbacks=[early_stopping, reduce_lr])

    histories.append(history)


# Calcular as métricas médias de validação cruzada (dos histories)
print("\n------------------------------------")
print("Cross-Validation Results:")
avg_val_loss = np.mean([h.history['val_loss'][-1] for h in histories if 'val_loss' in h.history]) # Pega o último val_loss de cada histórico
avg_val_accuracy = np.mean([h.history['val_accuracy'][-1] for h in histories if 'val_accuracy' in h.history]) # Pega o último val_accuracy de cada histórico
print(f"Average Validation Loss across {K} folds: {avg_val_loss:.4f}")
print(f"Average Validation Accuracy across {K} folds: {avg_val_accuracy:.4f}")
print("------------------------------------")


# Treinar o modelo final no conjunto de treino completo
print("\nTraining final model on the entire training dataset...")
final_model = create_model(num_classes)

# Criar o dataset de treino completo novamente
full_train_dataset = create_tf_dataset(image_paths, labels, num_classes, batch_size=32, shuffle=True, augment=True)

# Callbacks para o treinamento final
early_stopping_final = callbacks.EarlyStopping(
    monitor='loss', # Monitorar a perda de treino, pois não temos um conjunto de validação separado
    patience=10,
    restore_best_weights=True
)

reduce_lr_final = callbacks.ReduceLROnPlateau(
    monitor='loss', # Monitorar a perda de treino
    factor=0.2,
    patience=5,
    min_lr=0.00001
)

model_checkpoint_final = ModelCheckpoint(
    filepath='/content/drive/MyDrive/PROJETO IA 2024-2025/modelos_salvos/melhor_modelo_cnn_4classes_final.keras', # Salva o modelo final treinado no dataset completo
    monitor='loss', # Monitorar a perda de treino
    save_best_only=True,
    mode='min',
    verbose=1
)


history_final = final_model.fit(
    full_train_dataset,
    epochs=50, # Ajuste o número de épocas
    callbacks=[early_stopping_final, reduce_lr_final, model_checkpoint_final])


# Plotagem dos resultados do treinamento final
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history_final.history['accuracy'], label='accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Final Training Accuracy')
plt.legend(loc='lower right')

plt.subplot(1, 2, 2)
plt.plot(history_final.history['loss'], label='loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Final Training Loss')
plt.legend(loc='upper right')
plt.grid(True)
plt.tight_layout()
plt.show()


# Avaliação do modelo final no conjunto de teste
print("\nAvaliação do modelo final no conjunto de teste:")
loss, accuracy = final_model.evaluate(test_dataset)
print(f"Loss no teste: {loss:.4f}")
print(f"Acurácia no teste: {accuracy:.4f}")